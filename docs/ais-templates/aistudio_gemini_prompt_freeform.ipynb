{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwyTRdwB8aW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RXInneX6xx7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "kWIuwKG2_oWE",
        "outputId": "6fe699cc-2a19-4209-ba4d-9a7512a27492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading: https://storage.googleapis.com/generativeai-downloads/images/scones.jpg\n",
            "[\n",
            "    {\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"What's in this picture?\"\n",
            "            },\n",
            "            {\n",
            "                \"file_data\": {\n",
            "                    \"mime_type\": \"image/jpeg\",\n",
            "                    \"file_uri\": \"https://generativelanguage.googleapis.com/v1beta/files/44dke5hzs31g\"\n",
            "                }\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# import necessary modules.\n",
        "import base64\n",
        "import copy\n",
        "import json\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "try:\n",
        "    # The SDK will automatically read it from the GOOGLE_API_KEY environment variable.\n",
        "    # In Colab get the key from Colab-secrets (\"üîë\" in the left panel).\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Parse the arguments\n",
        "\n",
        "model = \"gemini-1.5-flash\"  # @param {isTemplate: true}\n",
        "contents_b64 = \"W3sicGFydHMiOiBbeyJ0ZXh0IjogIldoYXQncyBpbiB0aGlzIHBpY3R1cmU/In0sIHsiZmlsZV9kYXRhIjogeyJ1cmwiOiAiaHR0cHM6Ly9zdG9yYWdlLmdvb2dsZWFwaXMuY29tL2dlbmVyYXRpdmVhaS1kb3dubG9hZHMvaW1hZ2VzL3Njb25lcy5qcGciLCAibWltZV90eXBlIjogImltYWdlL2pwZWcifX1dfV0=\"  # @param {isTemplate: true}\n",
        "generation_config_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False\n",
        "\n",
        "# Convert and upload the files\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data, index):\n",
        "    \"\"\"Upload files to the Files API.\n",
        "\n",
        "    For each file, Google AI Studio either sent:\n",
        "    - a Google Drive ID,\n",
        "    - a URL,\n",
        "    - a file path, or\n",
        "    - The raw bytes (`inline_data`).\n",
        "\n",
        "    The API only understands `inline_data` or it's Files API.\n",
        "    This code, uploads files to the files API where the API can access them.\n",
        "    \"\"\"\n",
        "\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        path = tempfiles / str(index)\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"üìÅ Files\" in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "\n",
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "index = 0\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data, index)\n",
        "            index += 1\n",
        "\n",
        "import json\n",
        "print(json.dumps(contents, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDHIQAd-o7VKeDzfFwxZ66U3zaC3wWFWHM\"  # Or read from a file\n",
        "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "\n",
        "def upload_to_gemini(path, mime_type=None):\n",
        "  \"\"\"Uploads the given file to Gemini.\n",
        "\n",
        "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
        "  \"\"\"\n",
        "  file = genai.upload_file(path, mime_type=mime_type)\n",
        "  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
        "  return file\n",
        "\n",
        "def wait_for_files_active(files):\n",
        "  \"\"\"Waits for the given files to be active.\n",
        "\n",
        "  Some files uploaded to the Gemini API need to be processed before they can be\n",
        "  used as prompt inputs. The status can be seen by querying the file's \"state\"\n",
        "  field.\n",
        "\n",
        "  This implementation uses a simple blocking polling loop. Production code\n",
        "  should probably employ a more sophisticated approach.\n",
        "  \"\"\"\n",
        "  print(\"Waiting for file processing...\")\n",
        "  for name in (file.name for file in files):\n",
        "    file = genai.get_file(name)\n",
        "    while file.state.name == \"PROCESSING\":\n",
        "      print(\".\", end=\"\", flush=True)\n",
        "      time.sleep(10)\n",
        "      file = genai.get_file(name)\n",
        "    if file.state.name != \"ACTIVE\":\n",
        "      raise Exception(f\"File {file.name} failed to process\")\n",
        "  print(\"...all files ready\")\n",
        "  print()\n",
        "\n",
        "# Create the model\n",
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 40,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-pro\",\n",
        "  generation_config=generation_config,\n",
        "  system_instruction=\"Predict only a  team label without any reason or other details, just by giving the input as github issue title and github issue description in the prompt and passing a pdf file where it has details of the every team label along with 10 issues and description.\",\n",
        ")\n",
        "\n",
        "# TODO Make these files available on the local file system\n",
        "# You may need to update the file paths\n",
        "files = [\n",
        "  upload_to_gemini(\"Team labels.pdf\", mime_type=\"application/pdf\"),\n",
        "]\n",
        "\n",
        "# Some files have a processing delay. Wait for them to be ready.\n",
        "wait_for_files_active(files)\n",
        "\n",
        "chat_session = model.start_chat(\n",
        "  history=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict a team label for the issue title : \\\"repository_ctx.original_name is the empty string under WORKSPACE\\\" & Description : \\\"When building under WORKSPACE with Bazel 8.1.0 (and 8.1.0rc2), repository_ctx.original_name will be the empty string. This produces default BUILD targets with empty names inside generated repos (the use case from #24467), breaking the build.\\n\\nCommit 8bcfb06 introduced repository_ctx.original_name for release in 8.1.0 (first appearing in 8.1.0rc2), closing #24467 and #25121. In bazelbuild/rules_scala#1694, I introduced code like this to use it when available, which breaks when building with Bazel 8.1.0 and --enable_workspace:\\n\\n# Replace with rctx.original_name once all supported Bazels have it\\n\\\"name\\\": getattr(rctx, \\\"original_name\\\", rctx.attr.default_target_name),\\nIt would be good to have this field set regardless when using WORKSPACE, even though it would duplicate repository_ctx.name, to avoid having to write code like this (which does work, but is quite clumsy):\\n\\n\\\"name\\\": (\\n    getattr(rctx, \\\"original_name\\\", rctx.attr.default_target_name) or\\n    rctx.name\\n),\\nAn alternative would be to remove it from WORKSPACE builds altogether, but it would be great to have it work with WORKSPACE so people can just use repository_ctx.original_name directly and forget about it once Bazel 8 is the oldest supported version (i.e., so the getattr checks can go away sooner than later).\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"Based on the provided information, the predicted team label is:\\n\\n**team-ExternalDeps**\\n\\n**Reason:** The issue involves the interaction of external repositories (managed via WORKSPACE or similar mechanisms) with Bazel's module system, as indicated by:\\n\\n*   The mention of `repository_ctx.original_name` in a `WORKSPACE` context.\\n*   External repositories and their handling in `MODULE.bazel` files are relevant to the *External Dependencies* team.\\n*   The need to adapt existing code to handle the change introduced to fix external repo and their names.\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"Bad link order with CcSharedLibraryInfo.linker_input\\\" and github issue Description : \\\"When using cc_import in combination with cc_shared_library, the CcSharedLibraryInfo.linker_input will contain the .so files from both the cc_shared_library and cc_imports. A dependency graph with diamond dependency may then end up in having the same .so file specified in CcSharedLibraryInfo.linker_input from multiple targets.\\n\\nConsider the following example:\\n\\ncc_import X\\ncc_shared_library A depends on X\\ncc_shared_library B depends on X\\ncc_binary top depends on A and B\\nWhen linking top, the link order top.a, A.so, X.so, B.so is produced and wrong. When B depends on X, X.so should be placed behind B.so. The wanted link order is top.a, A.so, B.so, X.so.\\n\\ncc_shared_library.bzl contains the following code:\\n\\n        CcSharedLibraryInfo(\\n            dynamic_deps = merged_cc_shared_library_infos,\\n            exports = exports.keys(),\\n            link_once_static_libs = curr_link_once_static_libs_set,\\n            linker_input = cc_common.create_linker_input(\\n                owner = ctx.label,\\n                libraries = depset([linking_outputs.library_to_link] + precompiled_only_dynamic_libraries),\\n            ),\\n        ),\\nThis will create a single LinkerInput with all the .so files. When multiple LinkerInputs are put in a depset, the ordering of the individual .so files in the LinkerInput.libraries field will not change. On the contrary, cc_library produces multiple LinkerInputs with one .so file in each. In that case, the depset is able to reorder them on a higher level in the dependency graph.\\n\\nThis problem can (hopefully) be solved if CcSharedLibraryInfo.linker_input is changed into CcSharedLibraryInfo.linker_inputs where each LinkerInput contains a single LibraryToLink.\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Rules-CPP\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"Configurable //experimental-like semantics\\\" and github issue Description : \\\"Blaze treats the //experimental hierarchy specially. Bazel once did as well, but it eventually surprised someone because it wasn't documented and then got removed. AFAIK, the special handling boils down to four major things:\\n\\nExclusion from target pattern expansion like //...\\nGraph edges may be outgoing, but not incoming\\nIgnoring visibility checks\\nIgnoring testonly checks\\nWe would like to invoke this behavior externally, but to eliminate the surprise factor, do so in a configurable way like .bazelrc options or possibly even REPO.bazel directives. Conceptually, we can do all these independently, though one could also argue that the simplicity of controlling them all with one knob is preferable. Discussion is welcome so we can implement this the right way.\\n\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Configurability\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"Improve error for non-existent exec group\\\" and github issue Description : \\\"If you have a macro that expands to some build target and a test target like:\\n\\ndef foo(name, **kwargs):\\n    generate_file(name + \\\".gen\\\", **kwargs)\\n    test_something(name = name, file = name + \\\".gen\\\", **kwargs)\\nAnd you pass through exec_properties = {\\\"test.foo\\\": \\\"bar\\\"}, you get an error like this:\\n\\nrules_py/examples/pytest/BUILD.bazel:8:8: Tried to set properties for non-existent exec groups: test.\\nIn this case it's pretty unclear from the error message that the problem is an underlying build target, and might send you down the wrong path. I think ideally it would mention either the label of the rule that is missing the exec group, or the rule type\\n\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Core\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"Improve error for non-existent exec group\\\" and github issue Description : \\\"If you have a macro that expands to some build target and a test target like:\\n\\ndef foo(name, **kwargs):\\ngenerate_file(name + \\\".gen\\\", **kwargs)\\ntest_something(name = name, file = name + \\\".gen\\\", **kwargs)\\nAnd you pass through exec_properties = {\\\"test.foo\\\": \\\"bar\\\"}, you get an error like this:\\n\\nrules_py/examples/pytest/BUILD.bazel:8:8: Tried to set properties for non-existent exec groups: test.\\nIn this case it's pretty unclear from the error message that the problem is an underlying build target, and might send you down the wrong path. I think ideally it would mention either the label of the rule that is missing the exec group, or the rule type\\n\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Execution\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        files[0],\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-ExternalDeps\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"Coverage doesn't work for JVM targets with buildbarn FUSE workers\\\" and github issue Description : \\\" Running coverage on Java targets with buildbarn FUSE workers fails on compilation instrumentation post-processing with following\\n\\njava.nio.file.FileAlreadyExistsException: bazel-out/k8-fastbuild/bin/bazel-out/k8-fastbuild/bin/liblib.jar-coverage-metadata/com/cognite/Lib.class.uninstrumented\\nThe issue is that JacocoInstrumentationProcessor https://github.com/bazelbuild/bazel/blob/master/src/java_tools/buildjar/java/com/google/devtools/build/buildjar/instrumentation/JacocoInstrumentationProcessor.java#L93 is doing recursive directory traversal while also moving&creating files in same directories.\\n\\nPOSIX readdir doesn't specify if files added since the traversal start will be visible\\nhttps://pubs.opengroup.org/onlinepubs/007904875/functions/readdir_r.html\\n\\nIf a file is removed from or added to the directory after the most recent call to opendir() or rewinddir(), whether a subsequent call to readdir_r() returns an entry for that file is unspecified.\\n\\nIn buildbarn FUSE case they do show up, flow:\\n\\nfind uninstrumented .class file\\nmove it to .class.uninstrumented\\nwrite instrumented class back to .class (new file!)\\nremove .class.uninstrumented\\nfind .class file again (this time it's already instrumented)\\nmoving to .class.uninstrumented fails because destination already exists\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Rules-Java\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"Is bazel mod graph missing information?\\\" and github issue Description : \\\" I'm finding it difficult to understand why Bazel is selecting a particular version of a specific module, viz. protobuf 29.0.\\n\\nI have a repository with only these two files:\\n\\n.bazelversion:\\n8.0.0\\n\\nMODULE.bazel:\\nbazel_dep(name = \\\"rules_java\\\", version = \\\"8.6.1\\\")\\nbazel_dep(name = \\\"rules_jvm_external\\\", version = \\\"6.7\\\")\\nWhen running bazel mod graph I get:\\n\\n> bazel mod graph (condensed for readability)\\n<root> (@_)\\n‚îú‚îÄ‚îÄ‚îÄrules_java@8.6.1\\n‚îÇ   ‚îú‚îÄ‚îÄ‚îÄprotobuf@29.0\\n‚îÇ   ‚îú‚îÄ‚îÄ‚îÄrules_cc@0.0.16\\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄprotobuf@29.0 (*)\\n‚îî‚îÄ‚îÄ‚îÄrules_jvm_external@6.7\\n    ‚îú‚îÄ‚îÄ‚îÄrules_kotlin@1.9.6\\n    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄrules_proto@7.0.2\\n    ‚îÇ       ‚îú‚îÄ‚îÄ‚îÄprotobuf@29.0 (*)\\n\\n> bazel mod explain protobuf\\n<root> (@_)\\n‚îú‚îÄ‚îÄ‚îÄrules_java@8.6.1\\n‚îî‚îÄ‚îÄ‚îÄrules_jvm_external@6.7\\nrules_java@8.6.1 depends on protobuf@27.0 (link).\\n\\nrules_jvm_external@6.7 depends (dev) on protobuf@29.3 (link) (doesn't count).\\n\\nrules_cc@0.0.16 depends on protobuf@27.0 (link).\\n\\nrules_proto@7.0.2 depends on protobuf@27.1 (link).\\n\\nIt's not clear where version 29.0 comes from. How can a user find the specific module which is making the version of another module be selected? In case it comes from another transitive dependency, shouldn't that be represented in the graph?\\n\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-ExternalDeps\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : Reland \\\"Make Bazel itself build under an output base with Unicode characters\\\" and github issue Description : \\\"Consumers of an Immutable{List,Map,Set} cannot know whether it has been created from a source with unspecified iteration order, such as HashMap. Since ImmutableMap itself prescribes that it preserves insertion order, it's reasonable for consumers to expect a specified, reasonable, deterministic order. While the current implementation of HashMap in the OpenJDK still results in a deterministic order, this kind of \\\"sequenced-washing\\\" doesn't cause non-determinism, but it still runs the risk of having the order of elements change between JDK versions or vendors.\\n\\nThe locations changed in this PR have been identified by instrumenting the factory methods of the Immutable{List,Map,Set} classes to track whether they 1) have been created from an unsequenced collection and 2) are iterated over. I then manually picked out those cases in which I couldn't rule out that a change in order would be observable to users.\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Core\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title :  \\\"[8.2.0] Preserve insertion order in maps that are traversed\\\" and github issue Description :\\\"Consumers of an Immutable{List,Map,Set} cannot know whether it has been created from a source with unspecified iteration order, such as HashMap. Since ImmutableMap itself prescribes that it preserves insertion order, it's reasonable for consumers to expect a specified, reasonable, deterministic order. While the current implementation of HashMap in the OpenJDK still results in a deterministic order, this kind of \\\"sequenced-washing\\\" doesn't cause non-determinism, but it still runs the risk of having the order of elements change between JDK versions or vendors.\\n\\nThe locations changed in this PR have been identified by instrumenting the factory methods of the Immutable{List,Map,Set} classes to track whether they 1) have been created from an unsequenced collection and 2) are iterated over. I then manually picked out those cases in which I couldn't rule out that a change in order would be observable to users.\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Configurability\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title :  \\\"Support auth_patterns in source.json for Private Dependencies in Bazel Registry\\\" and github issue description : \\\"Bazel currently supports private repositories in http_archive() using the auth_patterns attribute, which allows authentication for URLs that require credentials. However, source.json in the Bazel registry does not support auth_patterns, making it difficult to use private repositories when fetching dependencies via bazel module.\\n\\nThis feature request proposes adding auth_patterns support in source.json, ensuring that authentication details can be properly forwarded to http_archive() when using private dependencies in private Bazel Central Registries.\\n\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Core\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\" Include target label in error message when referencing an unknown exec group\\\"  and github issue description : \\\"If you have a macro that expands to some build target and a test target like:\\n\\ndef foo(name, **kwargs):\\n    generate_file(name + \\\".gen\\\", **kwargs)\\n    test_something(name = name, file = name + \\\".gen\\\", **kwargs)\\nAnd you pass through exec_properties = {\\\"test.foo\\\": \\\"bar\\\"}, you get an error like this:\\n\\nrules_py/examples/pytest/BUILD.bazel:8:8: Tried to set properties for non-existent exec groups: test.\\nIn this case it's pretty unclear from the error message that the problem is an underlying build target, and might send you down the wrong path. I think ideally it would mention either the label of the rule that is missing the exec group, or the rule type\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Execution\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title :  \\\"Support auth_patterns in source.json for Private Dependencies in Bazel Registry\\\" and github issue description : \\\"Bazel currently supports private repositories in http_archive() using the auth_patterns attribute, which allows authentication for URLs that require credentials. However, source.json in the Bazel registry does not support auth_patterns, making it difficult to use private repositories when fetching dependencies via bazel module.\\n\\nThis feature request proposes adding auth_patterns support in source.json, ensuring that authentication details can be properly forwarded to http_archive() when using private dependencies in private Bazel Central Registries.\\n\\n\\\"\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-ExternalDeps\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"Include output path in Remote Asset API client errors\\\" \",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Remote-Exec\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \" \\nPredict only a  team label as result for the giving github issue title : \\\"Fall back to rctx.name if $original_name is empty\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-ExternalDeps\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\" Proxy Issue\\\" description : \\\"I tried to use bazel to compile tensorflow-addons on GH200 (aarch64). The cluster requires a proxy to access the external website. But I still got a proxy issue from bazel after set up the http and https proxy envs. The envs should be correct because github or wget works well.\\n\\n$ bazel build build_pip_pkg\\nERROR: Error computing the main repository mapping: Error accessing registry https://bcr.bazel.build/: Failed to fetch registry file https://bcr.bazel.build/modules/protobuf/21.7/MODULE.bazel: Unable to tunnel through proxy. Proxy returns \\\"HTTP/1.1 407 Proxy Authentication Required\\\"\\n\\nenv | grep 'http'\\nhttps_proxy=TheProxySever # I hid details of the proxy server.\\nHTTPS_PROXY=TheProxySever\\nHTTP_PROXY=TheProxySever\\nhttp_proxy=TheProxySever\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-ExternalDeps\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"  Transition to platforms without inheriting incoming configuration\\\" description : \\\"This FR proposes that platform-based transitions reset all unspecified flags and constraints to their default values rather than inheriting their state from the configuration that is active at the time of the transition.\\n\\nBackground\\nToday, all transitions inherit their initial state from the currently active configuration. This makes a lot of sense for transitions that intentionally flip a small handful of flags, and is critical for some tasks. For transitions on a platform, though, this behavior is less desirable. A platform specifies constraints and flags that define a configuration that will support the target platform. When writing a platform, the author has to make some assumptions about any unmentioned flags. Today, it's difficult to reason about the exact incoming configuration when a transition occurs, which means that it's entirely possible for a platform to be misconfigured due to the incoming configuration state.\\n\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Configurability\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"  Bazel 8 intermittently hangs with remote cache enabled\\\" description : \\\" Bazel has been randomly hanging for us after upgrading to v8. We're currently on v8.1.0 and still seeing hangs.\\n\\nThe hanging logs manifest as (4 actions, 0 running) and never progresses. If we disable remote caching then it appears to stop hanging.\\n\\nWhen we reverted to 7.5.0, the hangs went away.\\n\\n\\\"\\n\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Remote-Exec\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"   8.1.0: Unrecognized VM option 'UseAllWindowsProcessorGroups\\\" description : \\\" When invoked on Windows w/ BellSoft Liberica 21.0.4, Bazel 8.1.0 chokes w/ the following:\\n\\nUnrecognized VM option 'UseAllWindowsProcessorGroups'\\nError: Could not create the Java Virtual Machine.\\nError: A fatal exception has occurred. Program will exit.\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Core\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a  team label as result for the giving github issue title : \\\"    Make it possible to extend default_visibility for targets in a macro instead of override it\\\" description : \\\"  Today, if you want targets within a macro to always be visible to certain other targets, you can't also inherit default visibility.\\n\\nWhich category does this issue belong to?\\nLoading API\\n\\nWhat underlying problem are you trying to solve with this feature?\\nE.g. consider a macro like\\n\\ndef foo(name, visibility = None, **kw):\\n  _foo(name=name, visibility=visibility, **kw)\\nnow suppose I want the targets created by foo() to always be visible to \\\"//bar:__subpackages__\\\" - regardless of default_visibility. I can write\\n\\ndef foo(name, visibility = None, **kw):\\n  _foo(name=name, visibility=(visibility if visibility else []) + [\\\"//bar:__subpackages__\\\"], **kw)\\nbut this means that the following:\\n\\npackage(default_visibility = [\\\"//baz:__pkg__\\\"])\\nload(...)\\n\\nfoo(name=\\\"my_target\\\")\\n:my_target won't be visible to //baz:__pkg__ anymore, as the added visibility means we can't pass visibility=None and the default_visibility will then be ignored.\\n\\nToday this has to be manually handled by BUILD file authors or by BUILD file generators.\\n\\nIf bazel included a builtin function that could return the default visibility, e.g. native.package_default_visibility(), the macro could call that and use that:\\n\\ndef foo(name, visibility = None, **kw):\\n  _foo(name=name, visibility=(visibility if visibility else native.package_default_visibility()) + [\\\"//bar:__subpackages__\\\"], **kw)\\nwhich would get us the best of both worlds - macros could opt in to mutate visibility while also honoring default_visibility, without pushing the problem onto BUILD file authors or generators.\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Loading-API\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a team label as result for the given github issue title :\\\"Inconsistent behavior on cc_library.strip_include_prefix of Bazel 7+\\\" and description : \\\"When upgrading from Bazel 6.5.0 to Bazel 7.5.0, I found a potential undesired inconsistency behavior of cc_library's strip_include_prefix attribute.\\n\\nI'm using WORKSPACE (i.e. BzlMod is disabled).\\n\\nThe inconsistency causes behavior change when switching Bazel versions (6.5.0 and 7.5.0), and when working on the same code through local targets and remote targets in external repository.\\n\\nTo better demonstrate and describe the inconsistency, I have summarize a demo repository (inconsistency.tar.gz), and I have recorded the bahavior in src/BUILD.\\n\\nFrom my point of view, Bazel 6.5.0 works as expected, while Bazel 7.5.0 does not add -I to some virtual includes directory when using strip_include_prefix = \\\".\\\" on a cc_library target in an external repository.\\n\\nHowever, Bazel 7.5.0 processes correctly\\n\\nwhen using strip_include_prefix = \\\".\\\" on a local target;\\nwhen using strip_include_prefix = \\\"segment\\\" on both local or remote targets;\\nwhen using include_prefix = \\\".\\\" on a remote target;\\nThere are a number of opensource external repositories using strip_include_prefix = \\\".\\\" in their BUILD files, so the migration would be not easy. I wonder what's the original intention of the behavior change (or is it intentional from the beginning?), and if there is a way to configure Bazel 7.5.0 so that the breakage can be mitigated.\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Rules-CPP\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a team label as result for the given Github issue title : \\\" Fix encoding of Starlark source lines in error messages\\\" \",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Starlark-Integration\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a team label as result for the given Github issue title : \\\" reuse_sandbox_directories, experimental_sandbox_async_tree_delete_idle_threads may lead to Permission denied when test case creates a 0-bit directory \\\" and description : \\\"I'm upgrading our workspace from Bazel 7.4.0 to Bazel 8.1.0. My bazel test commands are quick to fail at random. Error messages mention Permission denied when trying to asynchronously clean up (or reuse?) sandbox trees.\\n\\nIt looks like one of our tests is misbehaving by creating a directory named TestDir-new at the root of its working directory (as opposed to beneath TEST_TMPDIR) w/ a mode of 0 (no read/write/execute bits).\\n\\nERROR: /path/to/WORKSPACE/package/subpackage/BUILD.bazel:88:10: Testing //package/subpackage:MyTest failed: Could not copy inputs into sandbox: /dev/shm/bazel-sandbox.9bc548904a956f24e976880d4b8012926278461cedcf42d6e9bf90d26f22ef55/linux-sandbox/15352/execroot/WORKSPACE/bazel-out/k8-dbg-obj/bin/package/subpackage/MyTest.runfiles/WORKSPACE/TestDir-new -> /dev/shm/bazel-sandbox.9bc548904a956f24e976880d4b8012926278461cedcf42d6e9bf90d26f22ef55/_moved_trash_dir/20057 (Permission denied)\\nBefore I discovered that TestDir-new came from our code, I was flailing around toggling flags, and I was able to mitigate this with either of the following:\\n\\n--experimental_sandbox_async_tree_delete_idle_threads=0\\n--noreuse_sandbox_directories\\nWhile this can be chalked up to user error, I'm still filing this only because we only encountered this failure mode when both of these features were enabled together. (At the very least, any other user encountering the same problem may find this report and infer an appropriate local fix.)\\n\\n\\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-Execution\\n\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"Predict only a team label as result for the given Github issue title : \\\"Allow silencing triaged warnings \\\" and description : \\\" When I invoke Bazel to build my project, I often get warnings from external dependencies. Some of these warnings are actionable: I appreciate them and fix them! Others I have reviewed and decided I should take no action on. Maybe the warning is a false positive (#7157, NB still not fixed!), or I'm already tracking fixing it in an issue and don't need it displayed to every developer on my project. I wish I could silence these triaged warnings.\\n\\nhttps://bazel.build/reference/command-line-reference#flag--ui_event_filters is the best we have today, but it's a blunt tool: AFAIK it only lets me silence all WARNING, DEBUG, or INFO messages. That's too coarse! I want to silence specific warnings that I've triaged.\\n\\n \\\"\",\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"team-CLI\\n\\n\",\n",
        "      ],\n",
        "    },\n",
        "  ]\n",
        ")\n",
        "\n",
        "response = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "g7v0zyLnkret",
        "outputId": "55c56339-ba4f-408d-fd56-23c7fe063fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'AIzaSyDHIQAd-o7VKeDzfFwxZ66U3zaC3wWFWHM'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-12c157e181c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerativeai\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AIzaSyDHIQAd-o7VKeDzfFwxZ66U3zaC3wWFWHM\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_to_gemini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmime_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'AIzaSyDHIQAd-o7VKeDzfFwxZ66U3zaC3wWFWHM'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}